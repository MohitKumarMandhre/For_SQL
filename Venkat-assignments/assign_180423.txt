solution 1
=>

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

default_args = {
    'owner': 'mohit',
    'start_date': datetime(2023, 4, 17),
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    'problem1',
    default_args=default_args,
    description='t1 followed by t2, a simple pipe',
    schedule_interval='@daily',
)

def task_1():
    print("Running Task 1")

def task_2():
    print("Running Task 2")

t1 = PythonOperator(
    task_id='task_1',
    python_callable=task_1,
    dag=dag,
)

t2 = PythonOperator(
    task_id='task_2',
    python_callable=task_2,
    dag=dag,
)

t1 >> t2

******************************************************************************

Solution 2
=>
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'mohit',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 17,21,40),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}
dag = DAG(
    'problem02',
    default_args=default_args,
    description='problem sattement two',
    schedule_interval=timedelta(days=1),
)
task1 = BashOperator(
    task_id='task1',
    bash_command='echo "Task 1"',
    dag=dag,
)
task2 = BashOperator(
    task_id='task2',
    bash_command='echo "Task 2"',
    dag=dag,
)
task3 = BashOperator(
    task_id='task3',
    bash_command='echo "Task 3"',
    dag=dag,
)

task1>>[task2,task3]

******************************************************************************

Solution 3
=>

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.postgres_operator import PostgresOperator

default_args = {
    'owner': 'mohit',
    'start_date': datetime(2023, 4, 17),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'problem 3',
    default_args=default_args,
    description='example of DAG using PostgresOperator',
    schedule_interval='@daily',
)

sql_query = """
    SELECT *
    FROM your_table
    WHERE date_col = '{{ ds }}'
"""

run_query = PostgresOperator(
    task_id='run_query',
    postgres_conn_id='your_postgres_conn_id',
    sql=sql_query,
    dag=dag,
)

run_query

******************************************************************************

Solution 4
=>

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

default_args = {
    'owner': 'mohit',
    'start_date': datetime(2023, 4, 17),
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    'problem1',
    default_args=default_args,
    description='a simple python pipe t1 followed by t2',
    schedule_interval='@daily',
)

def task_1():
    print("Running Task 1")

def task_2():
    print("Running Task 2")

t1 = PythonOperator(
    task_id='task_1',
    python_callable=task_1,
    dag=dag,
)

t2 = PythonOperator(
    task_id='task_2',
    python_callable=task_2,
    dag=dag,
)

t1 >> t2
